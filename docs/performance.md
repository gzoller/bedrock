# Performance Survey
In order to understand the relative benefits of the Bedrock tech stack we provide this simple performance survey across several popular REST endpoint tech stacks. It is important to note that these results are not intended to be absolute performance metrics. There are a lot of variables that go into real-world systems that are not adequately captured in a performance test like this. The intention here is to run a trivial endpoint in a constant, controlled environment to see how each of these popular frameworks perform relative to each other.

The environment of the test is not terribly relevant, so long as it is constant between runs and between platforms, but for the curious:
* Apple MacBook M2
* Test candidates run in a Docker container
* K6 generating load

The candidate platforms were selected as they represent popular choices in the industry for developing REST endpoints. These are:

* [Python with FastAPI](#python--fastAPI)
* [Node.js with Fastify](#nodejs--fastify)
* [Go with Gin](#go--gin)
* [Java with Spring Boot](#java--spring-boot )
* [Bedrock](#bedrock )

The raw stats are given as an appendix to this page for those who love tabular numbers. Performance summaries and analysis are given next. Finally a wrap-up of what all these metrics mean for actual systems.

## Python + FastAPI

### Outcome:
|Metric  |Python (FastAPI)  |Bedrock|Difference|
|--|--|--|--|
|Total Req Processed  |182,948  |577,869 | ðŸš€ Bedrock handled 3.16Ã— more requests|
|Avg Request Duration (http_req_duration)|1180 ms|22.90 ms|ðŸš€ Bedrock is 51.5Ã— faster  |
|Median Latency (med http_req_duration)|1100 ms|9.24 ms|ðŸš€ Bedrock median latency is 119Ã— lower|
|90th Percentile (p(90) http_req_duration)|2110 ms|63.35 ms |ðŸš€ FastAPIâ€™s p(90) is 33Ã— slower|
|99th Percentile (p(99) http_req_duration)|2390 ms|88.44 ms |ðŸš€ FastAPIâ€™s p(99) is 27Ã— slower|
|Max Latency (max http_req_duration) |5840 ms|403.65 ms|ðŸš€ FastAPIâ€™s worst-case latency is 14.5Ã— worse|
|Req per Second (http_reqs per sec) |3,631 req/sec|11,447 req/sec|ðŸš€ Bedrock handled 3.16Ã— more throughput|

### Analysis:
Bedrock is massively outperforming FastAPI under high load because:  
1. Pythonâ€™s FastAPI cannot handle the concurrency at 10,000 RPS.  
â€¢  Avg latency jumped to 1.18s (suggesting extreme queuing delays).  
â€¢  P(99) latency is 2.39s (many requests are taking multiple seconds).  
â€¢  Peak latency is 5.84s (some requests are waiting a long time).  
â€¢  FastAPI only handled 3,631 RPS before hitting its limits.  
2. Bedrock remains extremely fast and stable under load.  
â€¢  Avg latency is only 22.9ms (near instant).  
â€¢  P(99) latency is just 88ms (vs. FastAPIâ€™s 2.39s).  
â€¢  Max latency is only 403ms (vs. FastAPIâ€™s 5.84s).  
â€¢  Bedrock handled 11,447 RPS, which is 3.16Ã— the throughput of FastAPI.

**Why Is FastAPI So Much Slower?**

ðŸš¨ FastAPI is Likely Queuing Requests Because It Canâ€™t Handle Concurrency Efficiently  
â€¢  Pythonâ€™s GIL (Global Interpreter Lock) prevents true parallel execution.  
â€¢  FastAPIâ€™s asyncio event loop is overwhelmed, creating request backlog.  
â€¢  Requests are stacking up instead of being processed in parallel.

ðŸš¨ Why Is Bedrock So Much Faster?  
â€¢  JVM is highly optimized for multithreading.  
â€¢  Fibers are ultra-lightweight (can handle millions of concurrent requests).  
â€¢  Bedrock has lower overhead compared to FastAPIâ€™s asyncio event loop.


## Node.js + Fastify

### Outcome:
|Metric  |Node.js (Fastify)  |Bedrock|Difference|
|--|--|--|--|
|Total Req Processed  |378,344  |577,869 |ðŸš€ Bedrock handled 1.53Ã— more requests|
|Avg Request Duration (http_req_duration)|334.19 ms|22.90 ms|ðŸš€ Bedrock is 14.6Ã— faster|
|Median Latency (med http_req_duration)|188.26 ms|9.24 ms|ðŸš€ Bedrock median latency is 20Ã— lower|
|90th Percentile (p(90) http_req_duration)|502.15 ms|63.35 ms |ðŸš€ Fastify's p(90) is 8Ã— slower|
|99th Percentile (p(99) http_req_duration)|727.87 ms|88.44 ms |ðŸš€ Fastifyâ€™s p(99) is 8.2Ã— slower|
|Max Latency (max http_req_duration) |17040 ms|403.65 ms| ðŸš€ Fastifyâ€™s worst-case latency is 42Ã— worse|
|Req per Second (http_reqs per sec) |7,204 req/sec|11,447 req/sec|ðŸš€ Bedrock handled 1.58 more throughput|

### Analysis:
ðŸš€Bedrock outperforms Fastify in every key performance metric.
â€¢  Node.js Fastify exhibits much higher latencies under load.
â€¢  Bedrock processes more requests per second with much lower response times.
â€¢  Fastify struggles at high concurrency, especially at the 99th percentile and worst-case latencies.

**Why Is Fastify So Much Slower?**

ðŸš¨ Fastifyâ€™s Event Loop Queuing Delays Requests
â€¢  Fastify is single-threaded and relies on Node.jsâ€™s event loop.
â€¢  When requests stack up, they wait in the event loop queue instead of running in parallel.  
ðŸš¨ Fastify Struggles with High Parallelism
â€¢  Fastify is optimized for moderate concurrency but not extreme loads.
â€¢  At high concurrency (10,000 VUs), the event loop becomes a bottleneck.  
ðŸš¨ Garbage Collection & V8 Optimizations Arenâ€™t Enough
â€¢  Node.js uses V8â€™s GC, which is efficient, but not as optimized as the JVMâ€™s JIT compiler.
â€¢  Bedrock fibers handle much higher concurrency than Nodeâ€™s event loop.

**Why Is Bedrock So Much Faster?**

âœ… Bedrock Fibers Scale Better Than Node.js Event Loop
â€¢  Fibers run independently and can scale across multiple CPU cores.
â€¢  Bedrock does not rely on an event loop and schedules requests more efficiently.  
âœ… JVMâ€™s JIT Compiler & GC Outperform V8
â€¢  The JVMâ€™s Just-In-Time (JIT) compiler optimizes execution at runtime.
â€¢  Garbage collection (GC) in the JVM is more efficient at high loads than V8â€™s.  
âœ… Parallelism Works Natively in Bedrock
â€¢  Bedrock scales horizontally using fibers.
â€¢  Node.js, being single-threaded, struggles at higher concurrency levels.

## Go + Gin

### Outcome:
|Metric  |Node.js (Fastify)  |Bedrock|Difference|
|--|--|--|--|
|Total Req Processed  |299.294  |577,869 |ðŸš€ Bedrock handled 1.93Ã— more requests|
|Avg Request Duration (http_req_duration)|520.34 ms|22.90 ms|ðŸš€ Bedrock is 22.7Ã— faster|
|Median Latency (med http_req_duration)|344.3 ms|9.24 ms|ðŸš€ Bedrock is 37.3Ã— faster|
|90th Percentile (p(90) http_req_duration)|1180 ms|63.35 ms |ðŸš€ Bedrock's p(90) is 18.6Ã— faster|
|99th Percentile (p(99) http_req_duration)|1660 ms|88.44 ms |ðŸš€ Bedrock's p(99) is 18.8Ã— faster|
|Max Latency (max http_req_duration) |8510 ms|403.65 ms| ðŸš€ Bedrock's worst-case latency is 21Ã— better|
|Req per Second (http_reqs per sec) |5,927 req/sec|11,447 req/sec|ðŸš€ Bedrock had 93% higher throughput|

### Analysis:
ðŸš€ Bedrock significantly outperforms Go Gin under high concurrency.
â€¢  Go Gin struggles with high loads, leading to much higher latencies.
â€¢  Goâ€™s worst-case latency is 8.51s, while Bedrock remains under 500ms.
â€¢  Bedrock handles almost twice as many requests as Go Gin.
â€¢  At high concurrency, Go Gin does not scale as efficiently as Bedrock.

**Why Is Go Gin So Much Slower?**

ðŸš¨ 1. Goâ€™s Gorilla Mux Router Overhead
â€¢  Go Gin uses middleware layers for routing and request handling.
â€¢  Scalaâ€™s Bedrock is much leaner, avoiding heavy middleware.  
ðŸš¨ 2. Goâ€™s Standard Net/HTTP Library Handles Connections Differently
â€¢  Goâ€™s net/http library has a worker pool model that doesnâ€™t scale as efficiently.
â€¢  Bedrock's async I/O model allows much better parallelism.  
ðŸš¨ 3. Goâ€™s Garbage Collector (GC) May Be Slowing It Down
â€¢  Go uses a concurrent garbage collector, which can cause micro-latency spikes.
â€¢  Bedrock benefits from JVMâ€™s highly optimized GC and JIT compilation.  
ðŸš¨ 4. Go Goroutines vs. JVM Fibers
â€¢  Go Gin uses Goroutines, which are lightweight but not as optimized for high concurrency as JVM fibers.
â€¢  Bedrockâ€™s fibers are scheduled efficiently with better load balancing.

## Java + Spring Boot

### Outcome:
|Metric  |Node.js (Fastify)  |Bedrock|Difference|
|--|--|--|--|
|Total Req Processed  |552,498  |577,869 |ðŸš€ Bedrock handled 4.6% more requests|
|Avg Request Duration (http_req_duration)|25.59 ms|22.90 ms|ðŸš€ Bedrock is 11% faster|
|Median Latency (med http_req_duration)|13.33 ms|9.24 ms|ðŸš€ Bedrock is 1.4Ã— faster|
|90th Percentile (p(90) http_req_duration)|64.54 ms|63.35 ms |ðŸš€ Effectively identical|
|99th Percentile (p(99) http_req_duration)|88.44 ms|88.44 ms |ðŸš€ Effectively identical|
|Max Latency (max http_req_duration) |363.62 ms|403.65 ms| ðŸš€ Spring Boot has slightly better worst-case latency|
|Req per Second (http_reqs per sec) |7,693 req/sec|11,447 req/sec|ðŸš€ Bedrock handled 49% more throughput|

### Analysis:
ðŸš€ Bedrock and Spring Boot perform similarly at lower percentiles, but Bedrock scales better at high concurrency.
â€¢  Java Spring Boot and Bedrock both handle high concurrency well.
â€¢  At extreme load, Bedrock maintains a higher request rate (~49% more RPS).
â€¢  Spring Bootâ€™s worst-case latency is slightly better than Bedrock.
â€¢  Both frameworks have very close latency distributions up to the 99th percentile.

**Why Is Bedrock Faster?**

âœ… 1. Bedrock Uses a More Efficient Execution Model
â€¢  Bedrock fibers (ultra-lightweight threads) provide better parallelism than Java threads.
â€¢  Spring Boot uses standard Java threads, which are heavier and require more context switching.  
âœ… 2. Bedrock Has Lower Framework Overhead
â€¢  Spring Boot includes more built-in middleware, adding minor overhead.
â€¢  Bedrock is designed for minimal overhead and ultra-low-latency workloads.  
âœ… 3. Bedrock Uses a More Optimized Async Model
â€¢  Spring Bootâ€™s default async model (Tomcat thread pool) scales well but is not as optimized as Bedrock fibers.
â€¢  Bedrock can efficiently handle many concurrent requests with fewer resources.

**Why Is Spring Boot Still Performing Well?**

âœ… 1. JVM Optimizations Help Both
â€¢  Spring Boot and Bedrock both benefit from JVM JIT (Just-in-Time) compilation.
â€¢  Both handle memory and CPU well under load.  
âœ… 2. Spring Bootâ€™s Threading Model Works Well at Scale
â€¢  Spring Boot uses a traditional thread pool model, which is mature and well-optimized.
â€¢  Tomcat (or Netty) handles thousands of concurrent connections effectively.  
âœ… 3. Spring Boot Had Slightly Better Worst-Case Latency
â€¢  Bedrockâ€™s worst-case latency was slightly higher, likely due to the way fibers handle task scheduling.

## Bedrock
Throughout these tests, and in data not included here, one result is clear: If you are building low-throughput/low-traffic REST endpoints, the tech stack you choose doesn't really matter from a performance standpoint. They all perform reasonably well at lower levels of load. But... as load grows most of the platforms here hit a performance wall and start to degrade significantly. (Java + Spring Boot is the exception. It kept pace with higher loads rather well.)

What is revealing about this result is how prevalent Python and Node are in the marketplace given how poorly they perform at scale. (Go also suffers at scale but does not have the same presence as Node or Python in the marketplace.) What does this suggest? At Bedrock, it is our belief that companies choose the "script" (ie non-compiled) Node and Python languages because they're popular and easy to use and fast to deliver. Startups especially love these tech stacks. Initially, before the system has major load their platforms run well, however as user growth drives scale, these systems suffer and their owners are left with few options but to throw increasing spend at cloud vendors to run an ever-growing footprint of instances.

So what about Bedrock? The Bedrock tech stack maintains its efficiency and high performance, even under high loads. Even the 99% load remained sub-second. This is due to the extreme efficiencies of the fibre model in Bedrock. Node and Python are not even multi-threaded by default. These capabilities are added via 3rd party libraries like FastAPI. Bedrock goes even beyond threads (which Java/Spring Boot use) to a fibre model, which is even leaner and faster.

Perhaps your company or project started with an easy pick of platforms but now you're experiencing "the wall" of performance you can't seem to get around, aside from paying a lot of money to your cloud vendor. This is where you need Bedrock to reimagine your REST APIs on a much more performant platform, and one that offers many other benefits beyond  performance.

## Appendix: Raw Metrics
### Python + FastAPI
```
scenarios: (100.00%) 1 scenario, 10000 max VUs, 1m20s max duration (incl. graceful stop):  
         * default: Up to 10000 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)  
  
  
âœ“ status was 200  
  
checks.........................: 100.00% 182948 out of 182948  
data_received..................: 27 MB   527 kB/s  
data_sent......................: 16 MB   323 kB/s  
http_req_blocked...............: avg=10.67Âµs  min=0s       med=2Âµs  max=27.83ms  p(90)=4Âµs   p(95)=83Âµs  
http_req_connecting............: avg=7.05Âµs   min=0s       med=0s   max=27.82ms  p(90)=0s    p(95)=65Âµs  
http_req_duration..............: avg=1.18s    min=586Âµs    med=1.1s max=5.84s    p(90)=2.11s p(95)=2.39s  
  { expected_response:true }...: avg=1.18s    min=586Âµs    med=1.1s max=5.84s    p(90)=2.11s p(95)=2.39s  
http_req_failed................: 0.00%   0 out of 182948  
http_req_receiving.............: avg=137.02Âµs min=4Âµs      med=12Âµs max=146.63ms p(90)=357Âµs p(95)=562Âµs  
http_req_sending...............: avg=7.06Âµs   min=1Âµs      med=3Âµs  max=27.79ms  p(90)=14Âµs  p(95)=24Âµs  
http_req_tls_handshaking.......: avg=0s       min=0s       med=0s   max=0s       p(90)=0s    p(95)=0s  
http_req_waiting...............: avg=1.18s    min=562Âµs    med=1.1s max=5.84s    p(90)=2.11s p(95)=2.39s  
http_reqs......................: 182948  3631.397764/s  
iteration_duration.............: avg=1.69s    min=500.69ms med=1.6s max=6.34s    p(90)=2.61s p(95)=2.89s  
iterations.....................: 182948  3631.397764/s  
vus............................: 648     min=299              max=10000  
vus_max........................: 10000   min=10000            max=10000
```

### Node.js + Fastify
```
scenarios: (100.00%) 1 scenario, 10000 max VUs, 1m20s max duration (incl. graceful stop):  
         * default: Up to 10000 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)  
  
  
âœ“ status was 200  
  
checks.........................: 100.00% 378344 out of 378344  
data_received..................: 72 MB   1.4 MB/s  
data_sent......................: 34 MB   641 kB/s  
http_req_blocked...............: avg=5.16Âµs   min=0s       med=1Âµs      max=3.69ms p(90)=3Âµs      p(95)=4Âµs  
http_req_connecting............: avg=2.74Âµs   min=0s       med=0s       max=3.66ms p(90)=0s       p(95)=0s  
http_req_duration..............: avg=334.19ms min=451Âµs    med=188.26ms max=17.04s p(90)=502.15ms p(95)=727.87ms  
  { expected_response:true }...: avg=334.19ms min=451Âµs    med=188.26ms max=17.04s p(90)=502.15ms p(95)=727.87ms  
http_req_failed................: 0.00%   0 out of 378344  
http_req_receiving.............: avg=12.9Âµs   min=3Âµs      med=8Âµs      max=3.54ms p(90)=25Âµs     p(95)=33Âµs  
http_req_sending...............: avg=8.78Âµs   min=1Âµs      med=3Âµs      max=2.41ms p(90)=15Âµs     p(95)=27Âµs  
http_req_tls_handshaking.......: avg=0s       min=0s       med=0s       max=0s     p(90)=0s       p(95)=0s  
http_req_waiting...............: avg=334.17ms min=438Âµs    med=188.23ms max=17.04s p(90)=502.13ms p(95)=727.86ms  
http_reqs......................: 378344  7204.391064/s  
iteration_duration.............: avg=834.37ms min=500.51ms med=688.37ms max=17.54s p(90)=1s       p(95)=1.22s  
iterations.....................: 378344  7204.391064/s  
vus............................: 137     min=137              max=10000  
vus_max........................: 10000   min=10000            max=10000
```

### Go + Gin
```
scenarios: (100.00%) 1 scenario, 10000 max VUs, 1m20s max duration (incl. graceful stop):  
         * default: Up to 10000 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)  
  
  
âœ“ status was 200  
  
checks.........................: 100.00% 299294 out of 299294  
data_received..................: 43 MB   848 kB/s  
data_sent......................: 27 MB   528 kB/s  
http_req_blocked...............: avg=7.48Âµs   min=0s       med=2Âµs      max=7.61ms p(90)=4Âµs   p(95)=5Âµs  
http_req_connecting............: avg=4.75Âµs   min=0s       med=0s       max=7.57ms p(90)=0s    p(95)=0s  
http_req_duration..............: avg=520.34ms min=525Âµs    med=344.3ms  max=8.51s  p(90)=1.18s p(95)=1.66s  
  { expected_response:true }...: avg=520.34ms min=525Âµs    med=344.3ms  max=8.51s  p(90)=1.18s p(95)=1.66s  
http_req_failed................: 0.00%   0 out of 299294  
http_req_receiving.............: avg=13Âµs     min=4Âµs      med=9Âµs      max=3.12ms p(90)=26Âµs  p(95)=34Âµs  
http_req_sending...............: avg=8.56Âµs   min=1Âµs      med=3Âµs      max=5.88ms p(90)=16Âµs  p(95)=29Âµs  
http_req_tls_handshaking.......: avg=0s       min=0s       med=0s       max=0s     p(90)=0s    p(95)=0s  
http_req_waiting...............: avg=520.32ms min=495Âµs    med=344.28ms max=8.51s  p(90)=1.18s p(95)=1.66s  
http_reqs......................: 299294  5927.587864/s  
iteration_duration.............: avg=1.02s    min=500.62ms med=844.4ms  max=9.01s  p(90)=1.68s p(95)=2.16s  
iterations.....................: 299294  5927.587864/s  
vus............................: 700     min=283              max=9927  
vus_max........................: 10000   min=10000            max=10000
```

### Java + Spring Boot
```
scenarios: (100.00%) 1 scenario, 10000 max VUs, 1m20s max duration (incl. graceful stop):  
         * default: Up to 10000 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)  
  
  
âœ“ status was 200  
  
checks.........................: 100.00% 552498 out of 552498  
data_received..................: 74 MB   1.0 MB/s  
data_sent......................: 49 MB   687 kB/s  
http_req_blocked...............: avg=4.52Âµs   min=0s       med=1Âµs      max=16.45ms  p(90)=3Âµs      p(95)=4Âµs  
http_req_connecting............: avg=2.25Âµs   min=0s       med=0s       max=16.35ms  p(90)=0s       p(95)=0s  
http_req_duration..............: avg=25.59ms  min=459Âµs    med=13.33ms  max=363.62ms p(90)=64.54ms  p(95)=90.46ms  
  { expected_response:true }...: avg=25.59ms  min=459Âµs    med=13.33ms  max=363.62ms p(90)=64.54ms  p(95)=90.46ms  
http_req_failed................: 0.00%   0 out of 552498  
http_req_receiving.............: avg=15.67Âµs  min=3Âµs      med=7Âµs      max=25.01ms  p(90)=20Âµs     p(95)=28Âµs  
http_req_sending...............: avg=29.75Âµs  min=1Âµs      med=3Âµs      max=24.73ms  p(90)=30Âµs     p(95)=68Âµs  
http_req_tls_handshaking.......: avg=0s       min=0s       med=0s       max=0s       p(90)=0s       p(95)=0s  
http_req_waiting...............: avg=25.54ms  min=448Âµs    med=13.3ms   max=363.6ms  p(90)=64.49ms  p(95)=90.32ms  
http_reqs......................: 552498  7693.03046/s  
iteration_duration.............: avg=525.97ms min=500.52ms med=513.85ms max=863.89ms p(90)=564.94ms p(95)=590.94ms  
iterations.....................: 552498  7693.03046/s  
vus............................: 234     min=234              max=10000  
vus_max........................: 10000   min=10000            max=10000
```

### Bedrock
```
scenarios: (100.00%) 1 scenario, 10000 max VUs, 1m20s max duration (incl. graceful stop):  
         * default: Up to 10000 looping VUs for 50s over 3 stages (gracefulRampDown: 30s, gracefulStop: 30s)  
  
  
âœ“ status was 200  
  
checks.........................: 100.00% 577869 out of 577869  
data_received..................: 62 MB   1.2 MB/s  
data_sent......................: 51 MB   1.0 MB/s  
http_req_blocked...............: avg=9.36Âµs   min=0s       med=2Âµs      max=148.15ms p(90)=3Âµs      p(95)=4Âµs  
http_req_connecting............: avg=6.74Âµs   min=0s       med=0s       max=148.13ms p(90)=0s       p(95)=0s  
http_req_duration..............: avg=22.9ms   min=397Âµs    med=9.24ms   max=403.65ms p(90)=63.35ms  p(95)=88.44ms  
  { expected_response:true }...: avg=22.9ms   min=397Âµs    med=9.24ms   max=403.65ms p(90)=63.35ms  p(95)=88.44ms  
http_req_failed................: 0.00%   0 out of 577869  
http_req_receiving.............: avg=17.81Âµs  min=3Âµs      med=8Âµs      max=35.43ms  p(90)=19Âµs     p(95)=28Âµs  
http_req_sending...............: avg=46.94Âµs  min=1Âµs      med=5Âµs      max=31.02ms  p(90)=41Âµs     p(95)=95Âµs  
http_req_tls_handshaking.......: avg=0s       min=0s       med=0s       max=0s       p(90)=0s       p(95)=0s  
http_req_waiting...............: avg=22.84ms  min=375Âµs    med=9.18ms   max=390.4ms  p(90)=63.28ms  p(95)=88.28ms  
http_reqs......................: 577869  11446.896841/s  
iteration_duration.............: avg=523.53ms min=500.44ms med=509.94ms max=904.36ms p(90)=564.34ms p(95)=589.23ms  
iterations.....................: 577869  11446.896841/s  
vus............................: 514     min=313              max=9937  
vus_max........................: 10000   min=10000            max=10000
```